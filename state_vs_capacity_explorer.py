# -*- coding: utf-8 -*-
"""State_vs_Capacity_Explorer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/117BnIHKBNv7QooGU4MypcHgQdaQYFJPM
"""

import streamlit as st
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import copy

# ==========================================
# 1. THE COGNITIVE MODEL (PyTorch)
# ==========================================
class CognitiveRNN(nn.Module):
    """
    A Toy RNN where:
    - CAPACITY is set by hidden_size and num_layers (hard-coded at init).
    - STATE is applied dynamically during the forward pass (gain/noise).
    """
    def __init__(self, input_size=1, hidden_size=10, num_layers=1, output_size=1):
        super(CognitiveRNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # CAPACITY: The architectural constraints
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x, state_gain=1.0, state_noise=0.0):
        # 1. Recurrent Processing (The "Structure")
        # Initialize hidden state
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        out, _ = self.rnn(x, h0)

        # 2. State Modulation (The "Tuning")
        # Apply Global Gain (Neuromodulation/Arousal)
        out = out * state_gain

        # Apply Internal Noise (Stochasticity/Uncertainty)
        if state_noise > 0:
            noise = torch.randn_like(out) * state_noise
            out = out + noise

        # 3. Readout
        out = self.fc(out)
        return out

# ==========================================
# 2. DATA GENERATION
# ==========================================
def generate_task_data(seq_length=100):
    """Generates a complex non-linear wave (Sine + High Freq Cosine)."""
    t = np.linspace(0, 10, seq_length)
    # A wave that requires some memory/complexity to fit perfectly
    data = np.sin(t) + 0.5 * np.cos(3 * t)
    return t, torch.FloatTensor(data).view(1, -1, 1)

# ==========================================
# 3. STREAMLIT APP LOGIC
# ==========================================
st.set_page_config(page_title="State vs. Capacity Framework", layout="wide")

st.title("üß† State & Capacity: The Two-Axis Explorer")
st.markdown("""
**Interactive Supplement for:** *State and Capacity in Neural Models of Cognition*
This app demonstrates the dissociation between **Structural Capacity** (hardware/training) and **Computational State** (dynamic tuning).
""")

# Initialize Session State to hold the trained model
if 'model' not in st.session_state:
    st.session_state.model = None
if 'loss_history' not in st.session_state:
    st.session_state.loss_history = []
if 'trained_capacity' not in st.session_state:
    st.session_state.trained_capacity = "None"

# --- Sidebar: The Capacity Axis (Requires Training) ---
with st.sidebar:
    st.header("1. Capacity Axis (Structure)")
    st.info("Changing these requires retraining (Developmental time).")

    hidden_units = st.slider("RNN Hidden Units (Width)", 2, 64, 32, help="The dimensionality of the representational space.")
    layers = st.slider("RNN Layers (Depth)", 1, 3, 1, help="Hierarchical abstraction depth.")
    epochs = st.slider("Training Epochs", 50, 500, 200)
    lr = st.select_slider("Learning Rate", options=[0.001, 0.01, 0.05], value=0.01)

    train_btn = st.button("üèóÔ∏è Build & Train Model", type="primary")

# --- Main Area ---
col_L, col_R = st.columns([1, 2])

# --- Logic: Training ---
t_steps, X = generate_task_data()
Y = X.clone() # Auto-regressive task (predict next, but for vis we map Input->Output)

if train_btn:
    with col_R:
        progress_bar = st.progress(0)
        status_text = st.empty()

        # Init Model
        model = CognitiveRNN(hidden_size=hidden_units, num_layers=layers)
        optimizer = optim.Adam(model.parameters(), lr=lr)
        criterion = nn.MSELoss()

        history = []

        # Train Loop
        model.train()
        for i in range(epochs):
            optimizer.zero_grad()
            # During training, we assume "Optimal State" (Gain=1, Noise=0)
            outputs = model(X, state_gain=1.0, state_noise=0.0)
            loss = criterion(outputs, Y)
            loss.backward()
            optimizer.step()

            history.append(loss.item())

            if i % 10 == 0:
                progress_bar.progress(i / epochs)
                status_text.text(f"Training... Epoch {i}/{epochs} | Loss: {loss.item():.4f}")

        progress_bar.progress(1.0)
        status_text.success("Training Complete!")

        # Store in session
        st.session_state.model = model
        st.session_state.loss_history = history
        st.session_state.trained_capacity = f"{hidden_units} Units, {layers} Layers"

# --- Logic: Inference (State Manipulation) ---
with col_L:
    st.header("2. State Axis (Dynamics)")
    st.success("Tuning these is instant (Inference time).")

    # State Sliders
    gain = st.slider("Global Gain (Arousal)", 0.0, 3.0, 1.0, 0.1, help="Simulates neuromodulatory tone. <1.0 = Fatigue, >1.0 = Alert/Hyper.")
    noise = st.slider("Internal Noise (Stochasticity)", 0.0, 0.5, 0.0, 0.01, help="Simulates neural noise or uncertainty.")

    st.markdown("---")
    st.markdown("**Current Architecture:**")
    st.code(st.session_state.trained_capacity)

# --- Visualization ---
with col_R:
    st.subheader("Behavioral & Neural Output")

    if st.session_state.model is not None:
        model = st.session_state.model
        model.eval() # Switch to eval mode

        # Run Inference with LIVE State params
        with torch.no_grad():
            prediction = model(X, state_gain=gain, state_noise=noise)

        pred_np = prediction.view(-1).numpy()
        target_np = Y.view(-1).numpy()

        # Plotting
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), gridspec_kw={'height_ratios': [2, 1]})

        # Graph 1: The Task Performance
        ax1.set_title("Cognitive Task Performance (Wave Reconstruction)")
        ax1.plot(t_steps, target_np, 'k--', alpha=0.5, label="Target (World)")
        ax1.plot(t_steps, pred_np, 'b-', linewidth=2, label="Model (Cognition)")
        ax1.fill_between(t_steps, pred_np - noise, pred_np + noise, color='blue', alpha=0.1, label="Uncertainty")
        ax1.set_ylim(-2, 2)
        ax1.legend(loc='upper right')
        ax1.grid(True, alpha=0.3)

        # Graph 2: Training Loss (Capacity Check)
        ax2.set_title("Developmental Trajectory (Training Loss)")
        ax2.plot(st.session_state.loss_history, color='red')
        ax2.set_ylabel("MSE Loss")
        ax2.set_xlabel("Epochs")
        ax2.set_yscale("log")
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        st.pyplot(fig)

        # --- Diagnostic Text ---
        st.subheader("Diagnostic Interpretation")

        # Logic to generate "Paper-style" analysis
        final_loss = st.session_state.loss_history[-1]

        # 1. Check Capacity
        capacity_status = "Unknown"
        if final_loss > 0.1:
            st.error(f"üõë **Capacity Limit Reached:** The training loss is high ({final_loss:.3f}). The model structure (Hidden Units: {hidden_units}) is likely too simple to represent this complex wave. No amount of State tuning can fix this.")
        else:
            st.info(f"‚úÖ **Capacity Sufficient:** The model successfully learned the ontology (Loss: {final_loss:.3f}).")

            # 2. Check State (Only relevant if capacity is good)
            if gain < 0.6:
                st.warning("‚ö†Ô∏è **State Dysregulation (Low Gain):** The architecture is sound, but the signal is damped. This mimics fatigue or low vigilance.")
            elif gain > 1.5:
                st.warning("‚ö†Ô∏è **State Dysregulation (High Gain):** The system is over-amplifying inputs, potentially leading to instability.")
            elif noise > 0.2:
                st.warning("‚ö†Ô∏è **State Dysregulation (High Noise):** Representational precision is degraded by internal noise.")
            else:
                st.success("üéØ **Optimal Alignment:** The model is operating in a high-fidelity regime.")

    else:
        st.warning("üëà Please define Capacity and Click 'Train' to initialize the Cognitive Agent.")